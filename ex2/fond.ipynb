{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'mnist_data_handwritten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 74 106]\n",
      " [ 88 128]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define two 3D arrays\n",
    "a = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "b = np.array([[[1, 3], [5, 7]], [[2, 4], [6, 8]]])\n",
    "\n",
    "# Compute tensor dot product\n",
    "result = np.tensordot(a, b, axes=([1, 0], [0, 1]))\n",
    "\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T05:27:46.201821200Z",
     "start_time": "2024-01-14T05:27:46.021812100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def display_random_images(images, num_images=5):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(num_images):\n",
    "\n",
    "        idx = random.randint(0, len(images) - 1)\n",
    "        img = images[idx]\n",
    "\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(img, cmap=plt.cm.binary)\n",
    "        plt.xlabel(i)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    labels_path = os.path.join(path, f'{kind}-labels.idx1-ubyte')\n",
    "    images_path = os.path.join(path, f'{kind}-images.idx3-ubyte')\n",
    "\n",
    "    labels = read_idx(labels_path)\n",
    "    images = read_idx(images_path)\n",
    "\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "def data_preprocess(x,y):\n",
    "    x[x<=40]=0\n",
    "    x[x>40] =1\n",
    "    return x,y\n",
    "\n",
    "def normalize(image):\n",
    "    image -= image.min()\n",
    "    image = image / image.max()\n",
    "    image = image * 1.275 - 0.1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# 使用函数加载数据\n",
    "train_images, train_labels = load_mnist(data_path, kind='train')\n",
    "test_images, test_labels = load_mnist(data_path, kind='t10k')\n",
    "#display_random_images(train_images)\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict(test_images, model):\n",
    "    scores = model.output(test_images)\n",
    "    preds = np.argmax(scores, axis=1)\n",
    "    return preds\n",
    "\n",
    "def cal_accuracy(y_pred, y):\n",
    "    # TODO: Compute the accuracy among the test set and store it in acc\n",
    "\n",
    "    rs= 0\n",
    "    for _ in range(len(y)):\n",
    "        \n",
    "        if y_pred[_] == y[_]:\n",
    "            rs+=1\n",
    "    return rs/len(y)\n",
    "\n",
    "def get_acc(x,y,model):\n",
    "    preds = predict(x,model)\n",
    "    return cal_accuracy(preds,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax & cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # x [6000,10]\n",
    "\n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    return x_exp / (np.sum(x_exp,axis=1,keepdims=True))\n",
    "    \n",
    "def loss(y_hat,y):\n",
    "    if len(y.shape) >1 :\n",
    "        y = np.squeeze(y,axis=1)\n",
    "    #y_hat [6000.10] y[60000]\n",
    "    p = np.log(y_hat[list(range(len(y))),y]).mean()\n",
    "    return -p\n",
    "def cross_entropy(y_hat,y):\n",
    "    if len(y.shape) >1 :\n",
    "        y = np.squeeze(y,axis=1)\n",
    "    #y_hat [6000.10] y[60000]\n",
    "    p = np.log(y_hat[list(range(len(y))),y]).mean()\n",
    "    return -p\n",
    "\n",
    "def batch_generator(x,y,batch_size):\n",
    "    num = len(x)\n",
    "    for st in range(0,num,batch_size):\n",
    "        ed = min(st+batch_size,num)\n",
    "        yield x[st:ed], y[st:ed]\n",
    "\n",
    "def plpot(loss_values,acc_values):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # 创建新的图表窗口\n",
    "    plt.figure()\n",
    "\n",
    "    # 绘制 loss 曲线\n",
    "    plt.subplot(2, 1, 1)  # 创建一个 2x1 的图表网格，并选择第一个子图\n",
    "    plt.plot(loss_values, label='Loss')\n",
    "    plt.title('Loss over time')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # 绘制 accuracy 曲线\n",
    "    plt.subplot(2, 1, 2)  # 创建一个 2x1 的图表网格，并选择第二个子图\n",
    "    plt.plot(acc_values, label='Accuracy', color='orange')\n",
    "    plt.title('Accuracy over time')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    # 显示图表\n",
    "    plt.tight_layout()  # 自动调整子图参数，以保证图表不会重叠\n",
    "    plt.show()\n",
    "    #plt.savefig('theta_1000_0.05.png')\n",
    "\n",
    "\n",
    "def get_gradient(x,y,y_hat,theta):\n",
    "    reg_rate = 0.01\n",
    "    l = loss(y_hat, y) +  reg_rate * np.sum(theta*theta)\n",
    "    y_hat[list(range(len(y))),y]-=1\n",
    "    gradient = (y_hat.T) @ x \n",
    "\n",
    "    return gradient/len(x) ,l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多层感知器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Block:\n",
    "\n",
    "    def __init__(self, name=\"Block\") :\n",
    "        self.name = name\n",
    "        self.reg_rate = 0.05\n",
    "\n",
    "\n",
    "    def forwards(self,)->None:\n",
    "        pass\n",
    "    def backwards(self,)->None:\n",
    "        pass\n",
    "    def test_dims(self,)->None:\n",
    "        pass\n",
    "\n",
    "\n",
    "class MLP(Block):\n",
    "    def __init__(self, lname,dims, lr=0.001,bia=True):\n",
    "        super().__init__(name=\"MLP-\"+lname)\n",
    "        self.lr = lr\n",
    "        self.w = np.random.randn(*dims)\n",
    "        if bia:\n",
    "            self.bia = np.zeros(dims[-1])\n",
    "\n",
    "    \n",
    "    #forwards:\n",
    "    #input: [256, 120]  w: [120, 84]\n",
    "    #return [256, 84]\n",
    "    def forwards(self,x):\n",
    "        self.input = x\n",
    "        output = x @ self.w \n",
    "        if hasattr(self, 'bia'):\n",
    "            output += self.bia  \n",
    "        return output\n",
    "    \n",
    "    def backwards(self, dA):\n",
    "\n",
    "        #dA [256, 84]  dw[120, 84] = [256,120].T @ [256,84] \n",
    "        dW = self.input.T @ dA\n",
    "        dA = dA @ self.w.T\n",
    "\n",
    "        if hasattr(self, 'bia'):\n",
    "            db = np.sum(dA, axis=1).mean()\n",
    "\n",
    "        self.w -= self.lr * dW\n",
    "        if hasattr(self, 'bia'):\n",
    "            self.bia -= self.lr * db\n",
    "\n",
    "        return dA\n",
    "\n",
    "    def test(self, X):\n",
    "        self.forwards(X)\n",
    "        self.backwards(X)\n",
    "        print()\n",
    "        \n",
    "\n",
    "\n",
    "class Sigmoid(Block):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"Sigmoid\")\n",
    "\n",
    "    # Sigmoid函数\n",
    "    #input[256,1,n,n]\n",
    "    # return [256,1,n,n]\n",
    "    def forwards(self, x):\n",
    "        self.input = x\n",
    "        output = 1 / (1 + np.exp(-x))\n",
    "         \n",
    "        return output\n",
    "    \n",
    "    def backwards(self, dout):\n",
    "        sigmoid_derivative = self.input * (1 - self.input)  \n",
    "        dA = dout * sigmoid_derivative \n",
    "        return dA\n",
    "\n",
    "    def test(self):\n",
    "        \n",
    "        pass\n",
    "\n",
    "class Flatten(Block):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"Flatten\")\n",
    "\n",
    "    def forwards(self, x):\n",
    "        self.input_shape = x.shape  \n",
    "        output = x.reshape(x.shape[0], -1)\n",
    "        return output\n",
    "    \n",
    "    def backwards(self, dout):\n",
    "        dA = dout.reshape(self.input_shape) \n",
    "        return dA\n",
    "\n",
    "    \n",
    "    def test(self):\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "class SoftMax(Block):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"Softmax\")\n",
    "\n",
    "\n",
    "    def forwards(self, x):\n",
    "        self.input = x\n",
    "        self.output = softmax(x)\n",
    "        return self.output\n",
    "    \n",
    "    def backwards(self,y):\n",
    "        # 默认交叉熵\n",
    "        dA = self.output\n",
    "        dA[list(range(len(y))),y]-=1\n",
    "        #dA = (self.output.T) @ self.input \n",
    "        return dA \n",
    "    \n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self,name,model_list):\n",
    "        self.name = name\n",
    "        self.model_list  = model_list\n",
    "        \n",
    "\n",
    "    def output(self,x):\n",
    "        for layer in self.model_list:\n",
    "            x = layer.forwards(x)\n",
    "        return x\n",
    "\n",
    "    def backwards(self, y):\n",
    "        dA = y\n",
    "        for layer in self.model_list.__reversed__():\n",
    "                dA = layer.backwards(dA)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(X, pad):\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "    return X_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer(Block):\n",
    "    def __init__(self, lname,in_channels, out_channels, kernel_size, stride=1, padding=0, lr=0.005):\n",
    "        super().__init__(name=\"ConvLayer\"+lname)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.lr = lr\n",
    "\n",
    "        # 初始化权重和偏置\n",
    "        self.w = np.random.randn(kernel_size, kernel_size, in_channels, out_channels)\n",
    "        self.bias = np.zeros(out_channels)\n",
    "\n",
    "\n",
    "    def _simple_conv(self, x):\n",
    "        self.input = x\n",
    "\n",
    "        batch, in_channel, in_h, in_w = x.shape\n",
    "        out_h = (in_h + 2*self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_w = (in_w + 2*self.padding - self.kernel_size) // self.stride + 1\n",
    "        assert(out_h == out_w and in_channel == self.in_channels)\n",
    "  \n",
    "        if self.padding > 0:\n",
    "            X_padding = zero_padding(self.input,self.padding)\n",
    "            #input_padding[:, :, self.padding:-self.padding, self.padding:-self.padding]=self.input\n",
    "        else:\n",
    "            X_padding = self.input\n",
    "\n",
    "        output = np.zeros((batch, self.out_channels, out_h, out_w))\n",
    "\n",
    "        for i in range(batch):\n",
    "            for channel in range(self.out_channels):\n",
    "                for h in range(out_h):\n",
    "                    for w in range(out_w):\n",
    "                        h_start = h * self.stride\n",
    "                        h_end = h_start + self.kernel_size\n",
    "                        w_start = w * self.stride\n",
    "                        w_end = w_start + self.kernel_size\n",
    "\n",
    "                        #window = x[i, :, h_start:h_end, w_start:w_end]\n",
    "                        #output[i, channel, h, w] = np.sum(window * self.weights[channel]) + self.bias[channel]   \n",
    "                        conv_sum = 0\n",
    "                        # 对input map 的所有channel 同一位置卷积结果平均\n",
    "                        for m in range(in_channel): \n",
    "                 \n",
    "                            window = X_padding[i, m, h_start:h_end, w_start:w_end]\n",
    "                            \n",
    "                            conv_sum += np.sum(window * self.w[channel, m])\n",
    "\n",
    "                        output[i, channel, h, w] = conv_sum + self.bias[channel]\n",
    "          \n",
    "        return output\n",
    "\n",
    "    def _optimized_forward(self,x):\n",
    "        self.input = x\n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = x.shape\n",
    "        (f, f, n_C_prev, n_C) = self.w.shape\n",
    "\n",
    "        stride, pad = self.stride, self.padding\n",
    "\n",
    "        n_H = int((n_H_prev + 2 * pad - f) / stride + 1)\n",
    "        n_W = int((n_W_prev + 2 * pad - f) / stride + 1)\n",
    "\n",
    "        # Initialize the output volume Z with zeros.\n",
    "        Z = np.zeros((m, n_H, n_W, n_C))\n",
    "        A_prev_pad = zero_padding(x, pad)\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                # Use the corners to define the (3D) slice of a_prev_pad.\n",
    "                A_slice_prev = A_prev_pad[:, h * stride:h * stride + f, w * stride:w * stride + f, :]\n",
    "                # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron.\n",
    "                Z[:, h, w, :] = np.tensordot(A_slice_prev, self.w, axes=([1, 2, 3], [0, 1, 2])) + self.bias\n",
    "\n",
    "        assert (Z.shape == (m, n_H, n_W, n_C))\n",
    "        #cache = (A_prev, W, b, hyper_parameters)\n",
    "        return Z\n",
    "\n",
    "\n",
    "    def forwards(self, x):\n",
    "       return self._optimized_forward(x)\n",
    "\n",
    "    def _simple_bac(self, dout):\n",
    "        batch_size, _, in_height, in_width = self.input.shape\n",
    "        _, _, out_height, out_width = dout.shape\n",
    "\n",
    "        dW = np.zeros(self.w.shape)\n",
    "        dX_padded = np.zeros((batch_size, self.in_channels, in_height + 2 * self.padding, in_width + 2 * self.padding))\n",
    "        db = np.zeros(self.bias.shape)\n",
    "\n",
    "        \n",
    "        if self.padding > 0:\n",
    "            dX_padded = zero_padding(dX_padded,self.padding)\n",
    "            X_padded = zero_padding(self.input,self.padding)\n",
    "            #dX_padded[:, :, self.padding:-self.padding, self.padding:-self.padding] = self.input\n",
    "        else:\n",
    "            X_padded = self.input\n",
    "        \n",
    "        for i in range(batch_size): \n",
    "            for j in range(self.out_channels): \n",
    "                for k in range(self.in_channels):  \n",
    "                    for m in range(out_height): \n",
    "                        for n in range(out_width):  \n",
    "                            h_start,w_start = m * self.stride,n * self.stride\n",
    "                            window = X_padded[i, k, h_start:h_start + self.kernel_size, w_start:w_start + self.kernel_size]\n",
    "\n",
    "                            # 更新权重梯度\n",
    "                            dW[j, k] += window * dout[i, j, m, n]\n",
    "\n",
    "                            # 更新输入梯度\n",
    "                            dX_padded[i, k, h_start:h_start + self.kernel_size, w_start:w_start + self.kernel_size] += \\\n",
    "                                self.w[j, k] * dout[i, j, m, n]\n",
    "\n",
    "        for j in range(self.out_channels): \n",
    "            db[j] = np.sum(dout[:, j, :, :])\n",
    "\n",
    "        \n",
    "        if self.padding > 0:\n",
    "            dX = dX_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "        else:\n",
    "            dX = dX_padded\n",
    "\n",
    "        self.update(dW, db)\n",
    "        return dX\n",
    "\n",
    "    def _optimized_bac(self,dout):\n",
    "        A_prev, W, b = self.input,self.w, self.bias\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        f, f, n_C_prev, n_C = W.shape\n",
    "        m, n_H, n_W, n_C = dout.shape\n",
    "        stride,pad = self.stride, self.padding\n",
    "\n",
    "        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "        dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "        db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "        if pad != 0:\n",
    "            A_prev_pad = zero_padding(A_prev, pad)\n",
    "            dA_prev_pad = zero_padding(dA_prev, pad)\n",
    "        else:\n",
    "            A_prev_pad = A_prev\n",
    "            dA_prev_pad = dA_prev\n",
    "\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                # Find the corners of the current \"slice\"\n",
    "                vert_start, horiz_start = h * stride, w * stride\n",
    "                vert_end, horiz_end = vert_start + f, horiz_start + f\n",
    "\n",
    "                # Use the corners to define the slice from a_prev_pad\n",
    "                A_slice = A_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                # Update gradients for the window and the filter's parameters\n",
    "                dA_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end, :] += np.transpose(np.dot(W, dout[:, h, w, :].T), (3, 0, 1, 2))\n",
    "\n",
    "                dW += np.dot(np.transpose(A_slice, (1, 2, 3, 0)), dout[:, h, w, :])\n",
    "                db += np.sum(dout[:, h, w, :], axis=0)\n",
    "\n",
    "        # Set dA_prev to the unpadded dA_prev_pad\n",
    "        dA_prev = dA_prev_pad if pad == 0 else dA_prev_pad[:, pad:-pad, pad:-pad, :]\n",
    "\n",
    "        # Making sure your output shape is correct\n",
    "        assert (dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    " \n",
    "        return dA_prev\n",
    "\n",
    "    # dout[256 16 10 10]  -反卷积>  dw[16 5 5]   input [256 6 14 14]     \n",
    "    def backwards(self, dout):\n",
    "        \n",
    "        return self._optimized_bac(dout)\n",
    "\n",
    "    def update(self,dW, db,  momentum=0):\n",
    "        vw_u = - self.lr * dW\n",
    "        vb_u = - self.lr * db\n",
    "        self.w += vw_u\n",
    "        self.bias += vb_u\n",
    "        return\n",
    "\n",
    "class PoolingLayer(Block):\n",
    "    def __init__(self, pool_size, stride, mode='average'):\n",
    "        super().__init__(name=\"PoolingLayer\")\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        self.mode = mode\n",
    "\n",
    "    def _simple_for(self, x):\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        out_height = (height) // self.stride \n",
    "        out_width = (width ) // self.stride \n",
    "\n",
    "        output = np.zeros((batch_size, channels, out_height, out_width))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(channels):\n",
    "                for k in range(out_height):\n",
    "                    for l in range(out_width):\n",
    "                        h_start = k * self.stride\n",
    "                        w_start = l * self.stride\n",
    "                        window = x[i, j, h_start:h_start + self.pool_size, w_start:w_start + self.pool_size]\n",
    "\n",
    "                        if self.mode == 'max':\n",
    "                            output[i, j, k, l] = np.max(window)\n",
    "                        elif self.mode == 'average':\n",
    "                            output[i, j, k, l] = np.mean(window)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def _optimized_for(self, x):\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = x.shape\n",
    "        f,stride = self.pool_size, self.stride\n",
    "       \n",
    "        n_H = int(1 + (n_H_prev - f) / stride)\n",
    "        n_W = int(1 + (n_W_prev - f) / stride)\n",
    "        n_C = n_C_prev\n",
    "\n",
    "        A = np.zeros((m, n_H, n_W, n_C))\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                # Use the corners to define the current slice on the ith training example of A_prev, channel c\n",
    "                A_prev_slice = x[:, h * stride:h * stride + f, w * stride:w * stride + f, :]\n",
    "                # Compute the pooling operation on the slice. Use an if statement to differentiate the modes.\n",
    "                if self.mode == \"max\":\n",
    "                    A[:, h, w, :] = np.max(A_prev_slice, axis=(1, 2))\n",
    "                elif self.mode == \"average\":\n",
    "                    A[:, h, w, :] = np.average(A_prev_slice, axis=(1, 2))\n",
    "\n",
    "        # cache = (A_prev, hyper_parameters)\n",
    "        # assert (A.shape == (m, n_H, n_W, n_C))\n",
    "        return A\n",
    "\n",
    "    def forwards(self, x):\n",
    "        self.input = x\n",
    "        return self._optimized_for(x)\n",
    "\n",
    "\n",
    "    def _simple_bac(self, dout):\n",
    "        batch_size, channels, out_height, out_width = dout.shape\n",
    "        #out_height, out_width = dout.shape[2], dout.shape[3]\n",
    "        \n",
    "        dX = np.zeros_like(self.input)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(channels):\n",
    "                for k in range(out_height):\n",
    "                    for l in range(out_width):\n",
    "                        h_start = k * self.stride\n",
    "                        w_start = l * self.stride\n",
    "                        window = self.input[i, j, h_start:h_start + self.pool_size, w_start:w_start + self.pool_size]\n",
    "\n",
    "                        if self.mode == 'max':\n",
    "                            max_val = np.max(window)\n",
    "                            mask = (window == max_val)\n",
    "                            dX[i, j, h_start:h_start + self.pool_size, w_start:w_start + self.pool_size] += mask * dout[i, j, k, l]\n",
    "                        elif self.mode == 'average':\n",
    "                            average_val = dout[i, j, k, l] / (self.pool_size * self.pool_size)\n",
    "                            dX[i, j, h_start:h_start + self.pool_size, w_start:w_start + self.pool_size] += np.ones_like(window) * average_val\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def _optimized_bac(self, dout):\n",
    "        A_prev=self.input\n",
    "\n",
    "        stride, f = self.stride, self.pool_size\n",
    "\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape  # 256,28,28,6\n",
    "        m, n_H, n_W, n_C = dout.shape  # 256,14,14,6\n",
    "\n",
    "        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))  # 256,28,28,6\n",
    "\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                # Find the corners of the current \"slice\"\n",
    "                vert_start, horiz_start = h * stride, w * stride\n",
    "                vert_end, horiz_end = vert_start + f, horiz_start + f\n",
    "\n",
    "                # Compute the backward propagation in both modes.\n",
    "                if self.mode == \"max\":\n",
    "                    A_prev_slice = A_prev[:, vert_start: vert_end, horiz_start: horiz_end, :]\n",
    "                    A_prev_slice = np.transpose(A_prev_slice, (1, 2, 3, 0))\n",
    "                    mask = A_prev_slice == A_prev_slice.max((0, 1))\n",
    "                    mask = np.transpose(mask, (3, 2, 0, 1))\n",
    "                    dA_prev[:, vert_start: vert_end, horiz_start: horiz_end, :] \\\n",
    "                        += np.transpose(np.multiply(dout[:, h, w, :][:, :, np.newaxis, np.newaxis], mask), (0, 2, 3, 1))\n",
    "\n",
    "                elif self.mode == \"average\":\n",
    "                    da = dout[:, h, w, :][:, np.newaxis, np.newaxis, :]  # 256*1*1*6\n",
    "                    dA_prev[:, vert_start: vert_end, horiz_start: horiz_end, :] += np.repeat(np.repeat(da, 2, axis=1), 2, axis=2) / f / f\n",
    "\n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        return dA_prev\n",
    "    def backwards(self, dout):\n",
    "        return self._optimized_bac(dout)\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten  [8, 400]\n",
      "MLP-0  [8, 120]\n",
      "Sigmoid  [8, 120]\n",
      "MLP-1  [8, 84]\n",
      "Sigmoid  [8, 84]\n",
      "MLP-2  [8, 10]\n",
      "backwards\n",
      "MLP-2  [8, 84]\n",
      "Sigmoid  [8, 84]\n",
      "MLP-1  [8, 120]\n",
      "Sigmoid  [8, 120]\n",
      "MLP-0  [8, 400]\n",
      "Flatten  [8, 16, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((256,16,5,5))\n",
    "layers = [Flatten(), MLP(\"0\", (16*25,120)),Sigmoid(),MLP(\"1\",(120,84)),Sigmoid(),MLP(\"2\",(84,10))]\n",
    "\n",
    "shap = (8,16,5,5)\n",
    "x = np.random.randn(*shap)\n",
    "\n",
    "for l in layers:\n",
    "    x = l.forwards(x)\n",
    "    print(l.name + \" \" , list(x.shape)) \n",
    "    \n",
    "print('backwards')\n",
    "dA = x\n",
    "for l in layers.__reversed__():\n",
    "    dA = l.backwards(dA)\n",
    "    print(l.name + \" \" , list(dA.shape)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_regression(model, x, y, iters, batch_size=1):\n",
    "\n",
    "    if len(y.shape)>1:\n",
    "        y=np.squeeze(y)\n",
    "    #batch_size = 600\n",
    "    loss_ls = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    for epoch in range(iters):\n",
    "        loss_sum = 0\n",
    "        genertor = batch_generator(x, y, batch_size)\n",
    "        for i,(x_,y_) in enumerate(genertor):\n",
    "            #print(f\"epoch:{i}:\")\n",
    "            y_hat = model.output(x_)\n",
    "            l = cross_entropy(y_hat,y_)\n",
    "            loss_sum = loss_sum+l\n",
    "            model.backwards(y_)\n",
    "            \n",
    "            if not (((i+1)*batch_size))%1000:\n",
    "                loss_ls.append(l)\n",
    "                #acc_train.append(get_acc(x, y, model))\n",
    "                #print(f\"Loss:{l}\")\n",
    "        print(f\"Epoch{epoch}\\n Loss:{loss_sum/(60000)},acc:{get_acc(x,y,model)}\")\n",
    "\n",
    "    plpot(loss_ls,acc_train)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2583132500.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn [1], line 13\u001B[1;36m\u001B[0m\n\u001B[1;33m    layers =  [GFlatten(). <:P)\"\"A,(-28,2ii,2),SffaMax()]\u001B[0m\n\u001B[1;37m                           ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "layers = [Flatten(), MLP(\"0\", (28*28,10)),SoftMax()]\n",
    "model = Model(\"softmax\", layers)\n",
    "\n",
    "\n",
    "x,y = data_preprocess(train_images.copy(),train_labels.copy())\n",
    "\n",
    "#iter = batch_generator(train_images,train_labels,batch_size=256)\n",
    "softmax_regression(model,x,y,30,batch_size=600)\n",
    "ty,tyl = data_preprocess(test_images.copy(),test_labels.copy())\n",
    "get_acc(ty,tyl ,model)\n",
    "\n",
    "\n",
    "layers =  [GFlatten(). <:P)\"\"A,(-28,2ii,2),SffaMax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8901"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ty,tyl = data_preprocess(test_images.copy(),test_labels.copy())\n",
    "\n",
    "get_acc(ty,tyl ,model)\n",
    "layers = [latten(),MLP(\"0\", (29,202,10)), softmax()]\n",
    "\n",
    "model = Model(s\"saaq\", layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y, iters, batch_size=128):\n",
    "\n",
    "    #batch_size = 600\n",
    "    loss_ls = []\n",
    "    acc_train = []\n",
    "    acc_test = []\n",
    "\n",
    "    #print(f\"Epoch{-1}\\n acc:{get_acc(x,y,model)}\")\n",
    "    from tqdm import trange\n",
    "    for epoch in trange(iters):\n",
    "        loss_sum = 0\n",
    "        genertor = batch_generator(x, y, batch_size)\n",
    "        for i,(x_,y_) in enumerate(genertor):\n",
    "            #print(f\"epoch:{i}:\")\n",
    "            y_hat = model.output(x_)\n",
    "            l = cross_entropy(y_hat,y_)\n",
    "            loss_sum = loss_sum+l\n",
    "            model.backwards(y_)\n",
    "            \n",
    "            if not (((i+1)*batch_size))%1000:\n",
    "                loss_ls.append(l)\n",
    "                #acc_train.append(get_acc(x, y, model))\n",
    "                #print(f\"Loss:{l}\")\n",
    "        print(f\"Epoch{epoch}\\n Loss:{loss_sum/(60000)},acc:{get_acc(x,y,model)}\")\n",
    "\n",
    "    plpot(loss_ls,acc_train)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_22080\\1736245267.py:68: RuntimeWarning: overflow encountered in exp\n",
      "  output = 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_22080\\1736245267.py:41: RuntimeWarning: overflow encountered in matmul\n",
      "  dA = dA @ self.w.T\n",
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:181: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "  3%|▎         | 1/30 [00:00<00:11,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:00<00:11,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:01<00:10,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch2\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:01<00:10,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch3\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:02<00:09,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch4\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:02<00:09,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch5\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:02<00:09,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch6\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:03<00:08,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch7\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:03<00:08,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch8\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:04<00:08,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch9\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:04<00:07,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch10\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:04<00:07,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch11\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:05<00:06,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch12\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:05<00:06,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch13\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:05<00:05,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch14\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:06<00:05,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch15\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:06<00:05,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch16\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:07<00:04,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch17\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:07<00:04,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch18\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:08<00:04,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch19\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:08<00:03,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch20\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:08<00:03,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch21\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:09<00:02,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch22\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:09<00:02,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch23\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:09<00:01,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch24\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:10<00:01,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch25\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:10<00:01,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch26\n",
      " Loss:nan,acc:0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [32], line 26\u001B[0m\n\u001B[0;32m     24\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(x, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m#iter = batch_generator(train_images,train_labels,batch_size=256)\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mLeNet5\u001B[49m\u001B[43m,\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43miters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m ty,tyl \u001B[38;5;241m=\u001B[39m data_preprocess(test_images\u001B[38;5;241m.\u001B[39mcopy(),test_labels\u001B[38;5;241m.\u001B[39mcopy())\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Acc\u001B[39m\u001B[38;5;124m\"\u001B[39m,get_acc(ty,tyl ,model))\n",
      "Cell \u001B[1;32mIn [12], line 18\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, x, y, iters, batch_size)\u001B[0m\n\u001B[0;32m     16\u001B[0m l \u001B[38;5;241m=\u001B[39m cross_entropy(y_hat,y_)\n\u001B[0;32m     17\u001B[0m loss_sum \u001B[38;5;241m=\u001B[39m loss_sum\u001B[38;5;241m+\u001B[39ml\n\u001B[1;32m---> 18\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackwards\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (((i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m*\u001B[39mbatch_size))\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m1000\u001B[39m:\n\u001B[0;32m     21\u001B[0m     loss_ls\u001B[38;5;241m.\u001B[39mappend(l)\n",
      "Cell \u001B[1;32mIn [20], line 133\u001B[0m, in \u001B[0;36mModel.backwards\u001B[1;34m(self, y)\u001B[0m\n\u001B[0;32m    131\u001B[0m dA \u001B[38;5;241m=\u001B[39m y\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_list\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__reversed__\u001B[39m():\n\u001B[1;32m--> 133\u001B[0m         dA \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackwards\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdA\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [30], line 170\u001B[0m, in \u001B[0;36mConvolutionalLayer.backwards\u001B[1;34m(self, dout)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackwards\u001B[39m(\u001B[38;5;28mself\u001B[39m, dout):\n\u001B[1;32m--> 170\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimized_bac\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdout\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [30], line 154\u001B[0m, in \u001B[0;36mConvolutionalLayer._optimized_bac\u001B[1;34m(self, dout)\u001B[0m\n\u001B[0;32m    151\u001B[0m A_slice \u001B[38;5;241m=\u001B[39m A_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end, :]\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# Update gradients for the window and the filter's parameters\u001B[39;00m\n\u001B[1;32m--> 154\u001B[0m dA_prev_pad[:, vert_start:vert_end, horiz_start:horiz_end, :] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mtranspose(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdout\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m, (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m))\n\u001B[0;32m    156\u001B[0m dW \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(np\u001B[38;5;241m.\u001B[39mtranspose(A_slice, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m0\u001B[39m)), dout[:, h, w, :])\n\u001B[0;32m    157\u001B[0m db \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(dout[:, h, w, :], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "alpha = 0.0005\n",
    "\n",
    "layers = [ConvolutionalLayer(\"1->6\",1,6,5,stride=1,padding=2,lr=alpha),\n",
    "          Sigmoid(),\n",
    "          PoolingLayer(2,2,mode='average'),\n",
    "          ConvolutionalLayer(\"6->16\",6,16,5,stride=1,padding=0,lr=alpha),\n",
    "          Sigmoid(),\n",
    "          PoolingLayer(2,2,mode='average'),\n",
    "          Flatten(),\n",
    "          MLP(\"400->120\",(400,120),lr=alpha),\n",
    "          Sigmoid(),\n",
    "          MLP(\"120->84\",(120,84),lr=alpha),\n",
    "          Sigmoid(),\n",
    "          MLP(\"84->10\",(84,10),lr=alpha),\n",
    "          SoftMax()\n",
    "         ]\n",
    "\n",
    "LeNet5 = Model(name=\"Fond-LeNet\",model_list=layers)\n",
    "\n",
    "train_images, train_labels = load_mnist(data_path, kind='train')\n",
    "test_images, test_labels = load_mnist(data_path, kind='t10k')\n",
    "\n",
    "x,y = (normalize(train_images.copy()[:60]),  train_labels[:60])\n",
    "x = np.expand_dims(x, axis=-1)\n",
    "#iter = batch_generator(train_images,train_labels,batch_size=256)\n",
    "train(LeNet5,x,y,iters=30,batch_size=6)\n",
    "ty,tyl = data_preprocess(test_images.copy(),test_labels.copy())\n",
    "print(\"Test Acc\",get_acc(ty,tyl ,model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
